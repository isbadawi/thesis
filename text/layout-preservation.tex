A refactoring is a code transformation that changes the structure of the code
while preserving its semantics, and can often naturally be thought of as a
transformation over the structure of an abstract syntax tree. However, from
the perspective of a programmer using a refactoring tool, a refactoring is
ultimately a textual transformation. It is important to reconcile these two
conceptions of refactorings; purely working in terms of ASTs, while
technically correct from a semantics perspective, is apt to lose a lot
of information about the textual layout of the code, while purely working
in terms of text is apt to make implementations of individual refactorings
very brittle, hard to reason about, and hard to maintain.

In this chapter, we present our approach to the problem of implementing layout
preserving refactorings. We allow refactorings to be implemented in terms of a
minimalist tree transformation API, which hides the mechanics of layout
preservation. As refactoring writers specify tree-level changes, minimal
text-level changes are transparently computed from them. This simplifies the
implementations of individual refactorings, allowing them to remain oblivious
to the program text and to operate at a higher level of abstraction, without
compromising their practicality for end-users.

\section{Motivation}

Setting aside questions of semantics preservation, a refactoring is most
naturally thought of as a transformation on abstract syntax trees. For
instance, a refactoring like Extract Method can be boiled down to high-level
steps these:

\begin{enumerate}
  \item Synthesize a new function with the provided name
  \item Move the selected sequence of statements from the target function to
    the new function
  \item For each variable which is live at the input of the new function, add a
    corresponding input parameter to the function (or possibly or a global
    variable declaration)
  \item For each variable which is live at the end of the new function, add a
    corresponding output parameter to the function
  \item Synthesize a new function call with the appropriate number of input and
    output arguments
  \item Insert this function call in the target function, at the position of
    the original sequence of statements
\end{enumerate}

These steps are agnostic to program text. Given this, a natural structure for a
refactoring tool consists of parsing code, then performing transformations on
its AST, and then pretty printing the transformed AST to retrieve source code
to present to the programmer. Many refactoring tools -- particularly ones
developed for research purposes, where practicality is often a non-goal --
operate this way.

The problem with such approaches is that by construction the AST does not
contain enough information to accurately reconstruct the input source code.
Typically among the casualties are indentation and other whitespace, along
with comments and syntactic sugar.

\figref{Fig:LostLayout} shows a \matlab program and the result of parsing and
then pretty-printing it using the \mclab toolkit. The two programs are
behaviorally equivalent, but contain many syntactic differences. The comment
associated with the \code{cube} function has moved below the header. The
four-space indentation has been changed to two-space indentation. Each binary
expression has been wrapped in parentheses. The output parameters have been
wrapped in square brackets. The nested function \code{square} has been moved
-- in \matlab, nested functions have the same scope irrespective of where they
are declared, so during parsing they are all moved to the end of their enclosing
functions as a simplification step.

\begin{figure}
\begin{minipage}{0.5\linewidth}
\lstinputlisting[numbers=none]{code/cube_original.m}
\end{minipage}
\hfill \hspace{.3cm} \hfill
\begin{minipage}{0.5\linewidth}
\lstinputlisting[numbers=none]{code/cube_roundtrip.m}
\end{minipage}
\caption{An example of the lossy parsing and unparsing roundtrip.}
\label{Fig:LostLayout}
\end{figure}

Users of an automated refactoring tool are unlikely to be accepting of such
invasive changes to a program's text. As such, it is important for any
refactoring tool to be aware of the layout of the program when performing
refactorings. For a given AST transformation, it should endeavor to perform the
minimal textual changes needed to reflect the transformation in the program
text. In particular, unaffected portions of the program should not undergo any
textual changes.

Despite this, it is still convenient to express refactorings as tree
transformations. Refactorings would be much harder to implement and maintain if
they had to be expressed as textual transformations, or as a mixture of tree
and text transformations that had to be kept in sync.

Our goal is to be able to implement refactorings purely as tree
transformations, and to have minimal textual changes automatically computed
from them. In order to accomplish this, we introduce a simple transformation
API, which exposes a small set of tree manipulation operations. Instead of
directly manipulating AST nodes, refactorings are implemented in terms of this
API. Behind the scenes, the implementation of the API includes logic that keeps
track of the input program text in addition to the AST, and keeps the two in
sync.

\section{The transformation API}

Intuitively, all AST manipulation can be boiled down to a series of node
deletions and insertions. A replace operation is also convenient, and as we
will discuss further in the next section, our approach requires us to also take
on the responsibility of copying nodes. Finally, an operation to recover the
transformed source code is also needed. \figref{Fig:TransformerAPI} shows how
these operations are encoded as a Java interface. In order to demonstrate that
these operations are just conventional tree manipulation operations,
\figref{Fig:BasicTransformer} shows a trivial implementation of the interface
that just falls back on the operations exposed by the AST, ignoring layout
concerns; note the close correspondence between the two sets of operations.

\begin{figure}[htbp]
\lstinputlisting[numbers=none, language=Java]{code/Transformer.java}
\caption{The Transformer API, encoded as a Java interface}
\label{Fig:TransformerAPI}
\end{figure}

\begin{figure}
\lstinputlisting[numbers=none, language=Java]{code/BasicTransformer.java}
\caption{A trivial implementation of the Transformer API}
\label{Fig:BasicTransformer}
\end{figure}

\section{Synchronizing ASTs and token streams}

At a high level, our approach to layout preservation works as follows. Given a
\matlab source file, we start by tokenizing the source code using a \matlab
lexer, yielding a stream of tokens. Notably, this lexer does not drop any
tokens, preserving both whitespace and comments. Since some \matlab constructs
are whitespace-sensitive, and since keeping comments intact when compiling
\matlab to another language was considered a useful feature, the \mclab toolkit
already includes such a lexer; however, adapting this approach to other
languages may involve the use of a specialized lexer.

Alongside the token stream, we use a \matlab parser to parse the same source
file, yielding an abstract syntax tree. Our aim is to allow refactoring
writers to specify edits to the abstract syntax tree, and to have those edits
be transparently reflected in the token stream. In the end, when the time
comes to present the transformed source back to the user, we can simply
concatenate all the tokens instead of pretty printing the AST.

In order for this to work, there are two "primitives" that we rely on. First,
we need to be able to identify, for a particular AST node (which may be a node
from the original program, or a copy of a node, or a brand new synthesized
node), the portion of the token stream corresponding to that node. Second, we
need to be able to make local modifications to just that portion of the stream,
without compromising our ability to later look up nodes in the modified stream.

To bridge the gap between the token stream and the AST, we use position
information. Each token consists of a fragment of text together with a line and
column position where it occurs in the source code. Each AST node, assuming it
was produced by the parsing process and not manually synthesized after the
fact, also contains position information. As an initial link between the source
text and the AST, we can simply create a table that maps line and column
positions to the corresponding token in the stream. When we need to retrieve
the portion of the token stream for a given node, we can simply look up the
token corresponding to its start position, look up the node token corresponding
to its end position, and take all the tokens in between.

One potential complication here is that the mapping could become stale as the
token stream is modified. For instance, if we were to simply keep an array of
tokens and map positions to indices into the array, then as the stream is
edited and tokens are shifted around, indices would no longer point to the same
token. In some cases we may be able to update the mapping as we edit the
stream, but that approach quickly becomes brittle and hard to reason about.

In order to avoid this complication, and also to satisfy our requirement of
supporting local edits to the token stream, it is necessary to carefully
consider the data structure we will use to represent the token stream. A
natural choice is a doubly linked list. The values in our table can be
references to individual nodes in the list, which will remain valid even as
their positions within the list change. Also, since each node contains a
reference to its predecessor and successor within the list, we can cheaply
support edits like removing a sequence of tokens, or inserting a sequence of
tokens before or after a particular token.

A common operation when implementing refactorings is to copy an AST
node. Since this approach associates mutable state (a portion of the token
stream) with each node, it's not enough to simply copy the node; the
corresponding token stream fragment should be also be copied, and the copy
associated with the newly copied node, in order to ensure that we can clearly
distinguish the original from the copy and manipulate them independently.
This is the motivation for including the copy operation in the Transformer
interface shown in \figref{Fig:TransformerAPI}.

That suffices for correlating the token stream with the AST of the original
source text, but we need to be able to maintain this mapping as the token
stream is edited, code is moved around or copied, and new code is synthesized.

\section{Dealing with freshly synthesized code}

It is common for refactorings to insert new code into the program which wasn't
present in the original source text. For example, in the case of extract
method, a new function call is synthesized to replace the extracted statements,
a new function is synthesized to hold them, and that new function might also
contain some synthetic statements like global variable declarations to ensure
semantics are preserved. These pose a problem since there is no original text
to tokenize in this case.

The natural intuition is to somehow leverage the output of the pretty printer
to recover some text that we may integrate into the token stream. If we simply
pretty print the new AST, we get a program fragment as a string that we can
then feed to the lexer. However, since these synthetic AST nodes don't have
position information, we can't easily associate nodes with their portion of the
token stream. We can associate the top-level tree with the entire fragment, but
subtrees pose a problem.

One way to deal with this would be to pretty print the new AST to recover the
program text, then parse the text again to get back an AST that has position
information, and then proceed as before. This is viable, but it implies that
all new nodes would have to be synthesized through the tree transformation API
-- nodes that are synthesized by the caller directly couldn't be used, since
they wouldn't have the necessary position information.

In the interest of keeping the API surface small, we instead implement a
tokenizing pretty printer, which is a version of the pretty printer that emits
sequences of tokens instead of entire strings. This can implemented as a
straightforward recursive traversal of the AST; for each node, we synthesize a
sequence of tokens, and at the same time associate that sequence with the node
for later use.

\section{Putting it all together}

Given a program $P$, the lexer produces a stream $S$ of tokens $t_1, \dots,
t_n$. For a given token $t_i$ in $S$, we write $text(t_i)$ for the token's text
content, $startpos(t_i)$ for the token's starting position, and $endpos(t_i)$
for the token's ending position. Given the same $P$, the parser produces an AST
$T$. For each AST node $n$ in $T$, we overload our previous definitions and
write $startpos(n)$ for the node's starting position, and $endpos(n)$ for the
node's ending position. We take all of these as inputs; these are
conventionally provided by a typical lexer and parser.

We start by creating a doubly linked list $L$ with a node for each token in the
token stream. We write $head(L)$ and $tail(L)$ for the head and tail nodes of
$L$, respectively. Given a node $n$, we write $token(n)$ for the token
associated with $n$, $prev(n)$ for its predecessor node in $L$, and $next(n)$
for its successor node in $L$. We then define a mapping $P$ from source
positions to nodes in $L$; for each node $n$ in $L$, we map both
$startpos(token(n))$ and $endpos(token(n))$ to $n$.

A token stream fragment $f = \langle start(f), end(f) \rangle$ is a pair
of nodes in $L$: a starting node $start(f)$ and an ending node $end(f)$. Given
an AST node $n$ in $T$, we ultimately need to be able to retrieve (or
synthesize) a corresponding token stream fragment. For a node $n$ occurring in
the original AST produced by the parser, the corresponding token stream
fragment is $\langle P[startpos(n)], P[endpos(n)] \rangle$, but as alluded to
in the previous sections, this won't be accurate when dealing with synthetic
nodes, or copies of nodes. To handle these, we allow a node's token stream
fragment to be set explicitly, skipping the lookup in $P$. This facility is
used by the tokenizing pretty printer, and in the implementation of the copy
operation. We write $fragment(n)$ to retrieve the token stream fragment
associated with node $n$, if any.

Now when it comes to getting at a token stream fragment corresponding to a
given AST node $n$, we distinguish between the following cases.

\begin{enumerate}
  \item $n$ is a node from the original program, existing in the original
    AST produced by the parser. In that case, the corresponding fragment
    is just $\langle P[startpos(n)], P[endpos(n)] \rangle$.
  \item $n$ is a synthetic node we're seeing for the first time. In that case,
    we invoke the tokenizing pretty printer on $n$, which will output another
    doubly linked list $L'$. Now the corresponding fragment is
    $\langle head(L'), tail(L') \rangle$.
  \item a synthetic node we've seen before. In that case, the corresponding
    fragment is $fragment(n)$.
  \item a copy of another node. In this case, the correpsonding fragment is
    again $fragment(n)$.
\end{enumerate}

\section{Heuristics for handling indentation and comments}

Our approach of correlating AST nodes with fragments of the original program
text and performing local edits to the token stream lets us easily preserve the
layout of the unaffected portions of the code, as well as the internal layout
of the affected portions. However, care must still be taken when dealing with
code at the boundary between affected and unaffected. For instance, when
inserting a statement in the body of a deeply nested control structure, we
should pick an appropriate amount of indentation to match the surrounding code
-- but the program indentation might be inconsistent, or there might not be any
surrounding code in a particular file. Statements can also have comments
associated with them, and ideally these should be moved alongside their
subjects -- but since comments are largely free-form, there is no easy way to
identify which comments are associated with a given statement. In both cases,
we rely on heuristics to try and do something sensible.

\subsection{Comments}

In order to represent an association between a comment and a language
construct, we simply extend the token stream fragment associated with the
construct to include the comment. Note that this implies the portion of the
program text including the construct and the comment must be contiguous,
which might be problematic; for example, consider the (artificial) case
in \figref{Fig:WackyComments}, where multiple aligned inline comments are
meant to be associated with the first statement. We choose not to handle
such cases; at the least, it would disproportionately complicate the
implementation, as each node might have several disparate token stream
fragments associated with it.

\begin{figure}
\begin{lstlisting}[numbers=none, keepspaces=true]
function_call()  % This comment spans multiple lines
another_call()   % and lies adjacent to many statements
some_core_code() % but should be considered associated with
                 % the first function call.
\end{lstlisting}
\caption{An illustration of wacky commenting practices.}
\label{Fig:WackyComments}
\end{figure}

Comments can be associated with any kind of language construct, including
functions, scripts, classes, methods, statements and even individual
expressions. Comments associated with a node are also implicitly associated
with its ancestor nodes, so that, for instance, moving a function call also
moves any comments associated with its arguments. In each case, when an
operation is performed on a node, we inspect the surrounding code to gather any
associated comments. Given a node, our heuristic is as follows.

\begin{enumerate}
  \item For a statement or an expression, we include the trailing inline
    comment, if any. Given the node $n$, we take $end(fragment(n))$, which is a
    node in $L$, and search forwards along its successor nodes, skipping over
    any space or tab (but not newline) tokens. If we reach a comment, which the
    lexer will have grouped into a single token $t$, we replace
    $end(fragment(n))$ by $t$. Otherwise we'll reach either a newline, the end
    of the file, or some other text token, in which case we stop.
  \item For all nodes, we also include any preceding line comments. Given
    a node $n$, we take $start(fragment(n))$, which is a node in $L$, and
    search backwards along its predecessor nodes, skipping any whitespace
    tokens, including newline tokens. For each line comment token $t$ we
    encounter, we replace $start(fragment(n))$ by $t$ and continue, stopping
    only when we encounter a text token, or the start of the file.
\end{enumerate}

% TODO(isbadawi): Add figure showing which comments are tied to which node.

\subsection{Indentation}

When moving statements or functions, we attempt to match the surrounding
indentation. Given a simple statement node $n$, we compute its indentation
level by taking $start(fragment(n))$ and searching backwards along its
predecessor nodes until a non-whitespace or newline token $t$ is reached. Then
the token stream fragment corresponding to the indentation is \newline $\langle
next(t), prev(start(fragment(n))) \rangle$ (which is possibly empty). For a
multiline construct like a function or a loop, we compute the indentation of
each line and take the common prefix. When inserting a node, we try to guess
the appropriate amount of indentation to insert by applying the following
heuristic.

\begin{enumerate}
  \item Special case: if we're inserting a statement between two semicolon or
    comma separated statements on the same line, then there's no indentation to
    add.
  \item Look for a predecessor or successor AST node and copy its indentation.
  \item Look for a statement or function list elsewhere in the file, and copy
    the indentation of the first node there.
  \item Otherwise, there is nothing to go on -- a more sophisticated approach
    might inspect different files in the project, but each token stream is tied
    to a single file -- so we fall back to an arbitrary (potentially configurable)
    default of four spaces.
\end{enumerate}

\section{Niggling details: delimiters, parentheses}

In addition to moving existing pieces of code around and dealing with newly
created code, in some cases the transformation engine also needs to synthesize
new code.

Various \matlab language constructs are represented in the AST by delimited
lists. For instance, function bodies are newline or semicolon delimited lists
of statements, function definitions contain comma-delimited lists of input and
output parameter names, and array accesses or function calls contain comma or
space delimited lists of arguments. Since these delimiters are purely
syntactic, they are not represented in the AST. As such, in order to support
manipulating delimited lists, the transformation engine needs to transparently
deal with delimiters. For example, adding a second argument to a function call
-- represented in code by adding an expression node to a list of expressions --
requires inserting a comma before the text corresponding to the expression.
Similarly, removing an argument requires removing the corresponding comma.

Since the Transformer API takes all parameters as \texttt{ASTNode<?>}, the root
of the AST node class hierarchy, it doesn't necessarily know which delimiters
to use. One way to address this would be to have the Transformer API expose
different operations to manipulate statement lists, argument lists, and
parameter lists, but that would entail a much larger API surface -- a
disportionate cost for what should be a minor concern. Instead, the
transformation engine handles this by inspecting the AST fragments handed to
the \texttt{insert} and \texttt{remove} operations. By inspecting the structure
of the AST and performing some runtime type checks, we can distinguish between
the different cases we need to handle. For example, \figref{Fig:WhereAreWe}
shows a small method we can use to tell whether an arbitrary AST node actually
represents the input parameter list of a function; in that case, we would know
to use commas as delimiters, and also to surround the list with square brackets
as needed.

\begin{figure}
\begin{lstlisting}[language=Java, numbers=none]
private boolean isInputParamList(ASTNode<?> node) {
  return node.getParent() instanceof Function &&
      ((Function) node.getParent()).getInputParams() == node;
}
\end{lstlisting}
\caption{Using runtime type checks and AST traversal to distinguish cases.}
\label{Fig:WhereAreWe}
\end{figure}

Another case where the transformation engine may need to synthesize extra code
is the preservation of operator precedence. \figref{Fig:NeedParens} shows
a motivating example -- if we want to inline the \texttt{x} variable, the
expression on the right hand side needs to be wrapped in parentheses. This
should ideally be transparent to the calling code, where this should be simply
a call to the \texttt{replace} operation. As a simple heuristic to handle cases
like these, the transformation engine checks during the \texttt{copy} operation
whether the copied node is a binary expression, and wraps it with parentheses
if they're not already there.

\begin{figure}
\begin{minipage}{0.5\linewidth}
\lstinputlisting[numbers=none]{code/inline_parens_original.m}
\end{minipage}
\hfill \hspace{.3cm} \hfill
\begin{minipage}{0.5\linewidth}
\lstinputlisting[numbers=none]{code/inline_parens_transformed.m}
\end{minipage}
\caption{An example illustrating the need for synthesized parentheses.}
\label{Fig:NeedParens}
\end{figure}

\section{Case studies: inline variable, extract function}

The Inline Variable refactoring is relatively simple; it takes an assignment
statement as input, and replaces each use of the assigned variable with the
expression on the right hand side before removing the assignment.
\figref{Fig:InlineVariableBefore} shows a pre-existing implementation of this
refactoring (skipping over the portions of the code dealing with the
correctness of the transformation), and \figref{Fig:InlineVariableAfter} shows
how the mechanics of the transformation are implemented against the
transformation API.

% TODO(isbadawi): Add before-after figure, showing ASTs and token streams.

\begin{figure}
\lstinputlisting[numbers=none, language=Java]{code/InlineVariableBefore.java}
\caption{The original implementation of the Inline Variable refactoring}
\label{Fig:InlineVariableBefore}
\end{figure}

\begin{figure}
\lstinputlisting[numbers=none, language=Java]{code/InlineVariable.java}
\caption{The implementation of the Inline Variable refactoring using the transformer API}
\label{Fig:InlineVariableAfter}
\end{figure}

The Extract Function refactoring slightly more involved.
\figref{Fig:ExtractFunction} shows how the mechanics of the transformations are
implemented against the transformation API. Even though the refactoring moves
AST nodes around, copies nodes, and mixes in synthetic code with the original
text, the code is completely oblivious to text, instead leaning on the {\tt
Transformer} to do the heavy lifting.

\begin{figure}
\lstinputlisting[numbers=none, language=Java]{code/ExtractFunction.java}
\caption{The implementation of the Extract Function refactoring using the transformer API}
\label{Fig:ExtractFunction}
\end{figure}

\section{Related work}

The work that most closely resembles ours is HaRe, a refactoring tool for
Haskell \cite{HaRe}. It uses a similar approach of synchronizing an AST with a
token stream, which is then concatenated to produce transformed source code.

Waddington and Yao \cite{Proteus} tackled the same problem, which they termed
"the problem of style disruption", with Proteus, their refactoring tool for C
and C++. Their approach was to use a specialized AST called a "Literal-Layout
AST (LL-AST)", where literals, token and whitespace nodes are interspersed
alongside the regular nodes. Enhancing the AST with layout information has also
been the theme of a few other approaches to this problem \cite{RefactorErl}.

% TODO(isbadawi): Look at how http://scala-refactoring.org/ does it
% TODO(isbadawi): Look at "An Algorithm for Layout Preservation in Refactoring Transformations"
% TODO(isbadawi): Look at "Detaching and Reconstructing the Documentary Structure of Source Code" (looks iffy)

The Eclipse JDT contains infrastructure for modifying code at two levels -- a
lower-level API for describing text manipulation primitives, and a higher-level
AST rewriting API, which accepts descriptions of changes to AST nodes and uses
the text manipulation API to try and perform the textual changes required to
represent the AST changes. The approach is similar to ours in spirit; one big
difference is that since that our approach is implemented largely as a
standalone tool, we rely solely on lexing and parsing as primitives, while
Eclipse's implementation benefits from more sophisticated integration with a
scriptable text editor.
